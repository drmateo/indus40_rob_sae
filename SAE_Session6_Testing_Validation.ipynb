{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Session 6: System Testing and Performance Validation\n",
    "## Industry 4.0 Smart Manufacturing Cell - OpenManipulator-X\n",
    "\n",
    "**Duration**: 8 hours  \n",
    "**Prerequisites**: Sessions 1-5 completed  \n",
    "**Objectives**:\n",
    "- Develop comprehensive test suite for manufacturing cell\n",
    "- Implement performance metrics collection\n",
    "- Conduct repeatability and accuracy analysis\n",
    "- Validate system against specifications\n",
    "- Generate quality reports\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Unit Testing Framework (2 hours)\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Software Testing Levels**:\n",
    "1. **Unit Tests**: Individual functions/modules\n",
    "2. **Integration Tests**: Component interactions\n",
    "3. **System Tests**: Complete system behavior\n",
    "4. **Acceptance Tests**: Meets requirements\n",
    "\n",
    "For robotics systems, we also need:\n",
    "- **Kinematic validation**: FK/IK correctness\n",
    "- **Trajectory verification**: Smoothness, limits\n",
    "- **Performance benchmarks**: Speed, accuracy, reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from roboticstoolbox import DHRobot, RevoluteDH\n",
    "import spatialmath as sm\n",
    "import unittest\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Import your manufacturing cell modules\n",
    "# from kinematics import forward_kinematics, inverse_kinematics\n",
    "# from trajectory import plan_pick_trajectory, plan_inspection_trajectory\n",
    "# from vision import detect_parts, evaluate_quality\n",
    "# from control import ManufacturingCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.1: Kinematics Unit Tests\n",
    "\n",
    "Create comprehensive tests for FK and IK functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestKinematics(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    Unit tests for kinematics functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Initialize robot model before each test.\"\"\"\n",
    "        # TODO: Create robot instance\n",
    "        pass\n",
    "    \n",
    "    def test_fk_home_position(self):\n",
    "        \"\"\"\n",
    "        Test FK at home position [0, -45°, 45°, 0°].\n",
    "        Expected end-effector position should match manual calculation.\n",
    "        \"\"\"\n",
    "        q_home = np.array([0, np.deg2rad(-45), np.deg2rad(45), 0])\n",
    "        # TODO: Implement test\n",
    "        # T = forward_kinematics(q_home)\n",
    "        # self.assertAlmostEqual(T[0, 3], expected_x, places=3)\n",
    "        pass\n",
    "    \n",
    "    def test_fk_consistency_with_roboticstoolbox(self):\n",
    "        \"\"\"\n",
    "        Verify our FK matches RoboticsToolbox results.\n",
    "        \"\"\"\n",
    "        test_configs = [\n",
    "            [0, 0, 0, 0],\n",
    "            [np.pi/4, 0, 0, 0],\n",
    "            [0, np.pi/4, -np.pi/4, 0],\n",
    "        ]\n",
    "        # TODO: For each config, compare our FK with robot.fkine(q)\n",
    "        pass\n",
    "    \n",
    "    def test_ik_accuracy(self):\n",
    "        \"\"\"\n",
    "        Test IK by solving for known positions.\n",
    "        Strategy: FK(q) → T, then IK(T) → q', verify FK(q') ≈ T\n",
    "        \"\"\"\n",
    "        test_configs = [\n",
    "            [0, np.deg2rad(-45), np.deg2rad(45), 0],\n",
    "            [np.deg2rad(30), np.deg2rad(-20), np.deg2rad(30), 0],\n",
    "        ]\n",
    "        # TODO: Implement FK → IK → FK verification\n",
    "        pass\n",
    "    \n",
    "    def test_ik_multiple_solutions(self):\n",
    "        \"\"\"\n",
    "        Verify IK returns multiple solutions when they exist.\n",
    "        \"\"\"\n",
    "        # TODO: Test a pose with multiple IK solutions\n",
    "        # Check that all solutions give same end-effector pose\n",
    "        pass\n",
    "    \n",
    "    def test_ik_unreachable_pose(self):\n",
    "        \"\"\"\n",
    "        Verify IK correctly handles unreachable poses.\n",
    "        \"\"\"\n",
    "        # Target far outside workspace\n",
    "        T_unreachable = sm.SE3(1.0, 0, 0) # 1 meter away (impossible)\n",
    "        # TODO: Verify IK returns None or raises appropriate exception\n",
    "        pass\n",
    "    \n",
    "    def test_joint_limits(self):\n",
    "        \"\"\"\n",
    "        Verify joint limit checking.\n",
    "        \"\"\"\n",
    "        q_invalid = np.array([np.deg2rad(200), 0, 0, 0])  # Exceeds ±180°\n",
    "        # TODO: Verify function detects limit violation\n",
    "        pass\n",
    "\n",
    "# Run kinematics tests\n",
    "# suite = unittest.TestLoader().loadTestsFromTestCase(TestKinematics)\n",
    "# unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.2: Trajectory Tests\n",
    "\n",
    "Validate trajectory planning functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTrajectoryPlanning(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    Unit tests for trajectory planning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def test_trajectory_smoothness(self):\n",
    "        \"\"\"\n",
    "        Verify trajectories have continuous derivatives.\n",
    "        For quintic polynomials: q, qd, qdd should be continuous.\n",
    "        \"\"\"\n",
    "        waypoints = np.array([\n",
    "            [0, 0, 0, 0],\n",
    "            [1, 0.5, -0.5, 0],\n",
    "        ])\n",
    "        # TODO: Generate trajectory and check derivative continuity\n",
    "        pass\n",
    "    \n",
    "    def test_velocity_limits(self):\n",
    "        \"\"\"\n",
    "        Verify trajectory respects velocity limits.\n",
    "        \"\"\"\n",
    "        # TODO: Generate trajectory and check all velocities <= limits\n",
    "        pass\n",
    "    \n",
    "    def test_acceleration_limits(self):\n",
    "        \"\"\"\n",
    "        Verify trajectory respects acceleration limits.\n",
    "        \"\"\"\n",
    "        # TODO: Check accelerations\n",
    "        pass\n",
    "    \n",
    "    def test_waypoint_accuracy(self):\n",
    "        \"\"\"\n",
    "        Verify trajectory passes through all waypoints.\n",
    "        \"\"\"\n",
    "        # TODO: Check that trajectory visits each waypoint\n",
    "        pass\n",
    "    \n",
    "    def test_inspection_trajectory_circular(self):\n",
    "        \"\"\"\n",
    "        Verify inspection trajectory is actually circular.\n",
    "        \"\"\"\n",
    "        center = np.array([0.15, 0, 0.1])\n",
    "        radius = 0.05\n",
    "        # TODO: Generate inspection trajectory\n",
    "        # Check that all points are at distance 'radius' from 'center'\n",
    "        pass\n",
    "    \n",
    "    def test_collision_free_path(self):\n",
    "        \"\"\"\n",
    "        Verify collision-free path planning works.\n",
    "        \"\"\"\n",
    "        # TODO: Create obstacle, plan path, verify no collisions\n",
    "        pass\n",
    "\n",
    "# Run trajectory tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Performance Metrics Collection (2 hours)\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Key Performance Indicators (KPIs)** for manufacturing cell:\n",
    "\n",
    "1. **Cycle Time**: Total time per part\n",
    "2. **Throughput**: Parts per hour\n",
    "3. **Accuracy**: Position error at key points\n",
    "4. **Repeatability**: Variation across multiple cycles\n",
    "5. **Quality**: Percentage of correct classifications\n",
    "6. **Energy**: Power consumption per cycle\n",
    "7. **Reliability**: Success rate, failure modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMonitor:\n",
    "    \"\"\"\n",
    "    Collect and analyze performance metrics during operation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = {\n",
    "            'cycle_times': [],\n",
    "            'pick_times': [],\n",
    "            'inspect_times': [],\n",
    "            'place_times': [],\n",
    "            'energies': [],\n",
    "            'position_errors': [],\n",
    "            'quality_decisions': [],  # (actual, predicted)\n",
    "            'failures': [],\n",
    "            'timestamps': [],\n",
    "        }\n",
    "    \n",
    "    def record_cycle(self, cycle_data):\n",
    "        \"\"\"\n",
    "        Record data from one complete cycle.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        cycle_data : dict\n",
    "            {\n",
    "                'total_time': float,\n",
    "                'pick_time': float,\n",
    "                'inspect_time': float,\n",
    "                'place_time': float,\n",
    "                'energy': float,\n",
    "                'position_error': float,\n",
    "                'quality_actual': bool,\n",
    "                'quality_predicted': bool,\n",
    "                'success': bool,\n",
    "            }\n",
    "        \"\"\"\n",
    "        self.data['cycle_times'].append(cycle_data['total_time'])\n",
    "        self.data['pick_times'].append(cycle_data['pick_time'])\n",
    "        self.data['inspect_times'].append(cycle_data['inspect_time'])\n",
    "        self.data['place_times'].append(cycle_data['place_time'])\n",
    "        self.data['energies'].append(cycle_data['energy'])\n",
    "        self.data['position_errors'].append(cycle_data['position_error'])\n",
    "        self.data['quality_decisions'].append((cycle_data['quality_actual'], \n",
    "                                                cycle_data['quality_predicted']))\n",
    "        if not cycle_data['success']:\n",
    "            self.data['failures'].append({\n",
    "                'cycle': len(self.data['cycle_times']),\n",
    "                'reason': cycle_data.get('failure_reason', 'Unknown')\n",
    "            })\n",
    "        self.data['timestamps'].append(datetime.now())\n",
    "    \n",
    "    def compute_statistics(self):\n",
    "        \"\"\"\n",
    "        Compute performance statistics.\n",
    "        \"\"\"\n",
    "        n_cycles = len(self.data['cycle_times'])\n",
    "        \n",
    "        if n_cycles == 0:\n",
    "            return None\n",
    "        \n",
    "        # Time statistics\n",
    "        cycle_times = np.array(self.data['cycle_times'])\n",
    "        \n",
    "        # Quality statistics\n",
    "        quality_data = self.data['quality_decisions']\n",
    "        true_positives = sum(1 for actual, pred in quality_data if actual and pred)\n",
    "        true_negatives = sum(1 for actual, pred in quality_data if not actual and not pred)\n",
    "        false_positives = sum(1 for actual, pred in quality_data if not actual and pred)\n",
    "        false_negatives = sum(1 for actual, pred in quality_data if actual and not pred)\n",
    "        \n",
    "        stats = {\n",
    "            'n_cycles': n_cycles,\n",
    "            'cycle_time_mean': np.mean(cycle_times),\n",
    "            'cycle_time_std': np.std(cycle_times),\n",
    "            'cycle_time_min': np.min(cycle_times),\n",
    "            'cycle_time_max': np.max(cycle_times),\n",
    "            'throughput_per_hour': 3600 / np.mean(cycle_times) if np.mean(cycle_times) > 0 else 0,\n",
    "            'energy_mean': np.mean(self.data['energies']),\n",
    "            'energy_total': np.sum(self.data['energies']),\n",
    "            'position_error_mean': np.mean(self.data['position_errors']),\n",
    "            'position_error_max': np.max(self.data['position_errors']),\n",
    "            'accuracy': (true_positives + true_negatives) / n_cycles,\n",
    "            'precision': true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0,\n",
    "            'recall': true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0,\n",
    "            'false_positive_rate': false_positives / n_cycles,\n",
    "            'false_negative_rate': false_negatives / n_cycles,\n",
    "            'success_rate': 1 - len(self.data['failures']) / n_cycles,\n",
    "            'n_failures': len(self.data['failures']),\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def generate_report(self, filename='performance_report.json'):\n",
    "        \"\"\"\n",
    "        Generate detailed performance report.\n",
    "        \"\"\"\n",
    "        stats = self.compute_statistics()\n",
    "        \n",
    "        report = {\n",
    "            'test_date': datetime.now().isoformat(),\n",
    "            'statistics': stats,\n",
    "            'raw_data': {\n",
    "                'cycle_times': self.data['cycle_times'],\n",
    "                'energies': self.data['energies'],\n",
    "                'position_errors': self.data['position_errors'],\n",
    "            },\n",
    "            'failures': self.data['failures'],\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def plot_performance(self):\n",
    "        \"\"\"\n",
    "        Create performance visualization.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        # Cycle time over runs\n",
    "        axes[0, 0].plot(self.data['cycle_times'])\n",
    "        axes[0, 0].axhline(np.mean(self.data['cycle_times']), color='r', linestyle='--', label='Mean')\n",
    "        axes[0, 0].set_xlabel('Cycle Number')\n",
    "        axes[0, 0].set_ylabel('Cycle Time (s)')\n",
    "        axes[0, 0].set_title('Cycle Time per Run')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Time breakdown (pie chart)\n",
    "        time_breakdown = [\n",
    "            np.mean(self.data['pick_times']),\n",
    "            np.mean(self.data['inspect_times']),\n",
    "            np.mean(self.data['place_times']),\n",
    "        ]\n",
    "        axes[0, 1].pie(time_breakdown, labels=['Pick', 'Inspect', 'Place'], autopct='%1.1f%%')\n",
    "        axes[0, 1].set_title('Average Time Breakdown')\n",
    "        \n",
    "        # Energy consumption\n",
    "        axes[0, 2].plot(self.data['energies'])\n",
    "        axes[0, 2].set_xlabel('Cycle Number')\n",
    "        axes[0, 2].set_ylabel('Energy (arb. units)')\n",
    "        axes[0, 2].set_title('Energy per Cycle')\n",
    "        axes[0, 2].grid(True)\n",
    "        \n",
    "        # Position errors\n",
    "        axes[1, 0].plot(self.data['position_errors'])\n",
    "        axes[1, 0].axhline(2.0, color='r', linestyle='--', label='Spec Limit (2mm)')\n",
    "        axes[1, 0].set_xlabel('Cycle Number')\n",
    "        axes[1, 0].set_ylabel('Position Error (mm)')\n",
    "        axes[1, 0].set_title('Positioning Accuracy')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Cycle time histogram\n",
    "        axes[1, 1].hist(self.data['cycle_times'], bins=20, edgecolor='black')\n",
    "        axes[1, 1].set_xlabel('Cycle Time (s)')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].set_title('Cycle Time Distribution')\n",
    "        axes[1, 1].grid(True, axis='y')\n",
    "        \n",
    "        # Confusion matrix\n",
    "        quality_data = self.data['quality_decisions']\n",
    "        tp = sum(1 for actual, pred in quality_data if actual and pred)\n",
    "        tn = sum(1 for actual, pred in quality_data if not actual and not pred)\n",
    "        fp = sum(1 for actual, pred in quality_data if not actual and pred)\n",
    "        fn = sum(1 for actual, pred in quality_data if actual and not pred)\n",
    "        \n",
    "        confusion = np.array([[tn, fp], [fn, tp]])\n",
    "        im = axes[1, 2].imshow(confusion, cmap='Blues')\n",
    "        axes[1, 2].set_xticks([0, 1])\n",
    "        axes[1, 2].set_yticks([0, 1])\n",
    "        axes[1, 2].set_xticklabels(['Defect', 'Good'])\n",
    "        axes[1, 2].set_yticklabels(['Defect', 'Good'])\n",
    "        axes[1, 2].set_xlabel('Predicted')\n",
    "        axes[1, 2].set_ylabel('Actual')\n",
    "        axes[1, 2].set_title('Quality Classification')\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                text = axes[1, 2].text(j, i, confusion[i, j],\n",
    "                                      ha=\"center\", va=\"center\", color=\"black\", fontsize=16)\n",
    "        \n",
    "        plt.colorbar(im, ax=axes[1, 2])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('performance_analysis.png', dpi=150)\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "monitor = PerformanceMonitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.3: Run Performance Tests\n",
    "\n",
    "Execute multiple cycles and collect performance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_performance_test(n_cycles=50):\n",
    "    \"\"\"\n",
    "    Run manufacturing cell for n_cycles and collect metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_cycles : int\n",
    "        Number of parts to process\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    monitor : PerformanceMonitor\n",
    "        Monitor with collected data\n",
    "    \"\"\"\n",
    "    monitor = PerformanceMonitor()\n",
    "    \n",
    "    # TODO: Initialize manufacturing cell\n",
    "    # cell = ManufacturingCell(...)\n",
    "    \n",
    "    for i in range(n_cycles):\n",
    "        # Generate random part (with random quality)\n",
    "        part_quality_actual = np.random.rand() > 0.08  # 92% good rate\n",
    "        \n",
    "        # Simulate part detection\n",
    "        part_pose = {\n",
    "            'position': [0.2 + np.random.normal(0, 0.001), \n",
    "                        np.random.normal(0, 0.001),\n",
    "                        -0.05],\n",
    "            'orientation': np.random.uniform(-np.pi, np.pi)\n",
    "        }\n",
    "        \n",
    "        # TODO: Execute pick-inspect-place cycle\n",
    "        # result = cell.execute_cycle(part_pose)\n",
    "        \n",
    "        # Simulate result for now\n",
    "        result = {\n",
    "            'total_time': 8.0 + np.random.normal(0, 0.5),\n",
    "            'pick_time': 3.0 + np.random.normal(0, 0.2),\n",
    "            'inspect_time': 2.0 + np.random.normal(0, 0.1),\n",
    "            'place_time': 2.5 + np.random.normal(0, 0.2),\n",
    "            'energy': 50.0 + np.random.normal(0, 5),\n",
    "            'position_error': abs(np.random.normal(0, 0.5)),  # mm\n",
    "            'quality_actual': part_quality_actual,\n",
    "            'quality_predicted': part_quality_actual if np.random.rand() > 0.02 else not part_quality_actual,  # 98% accuracy\n",
    "            'success': np.random.rand() > 0.02,  # 98% success rate\n",
    "        }\n",
    "        \n",
    "        monitor.record_cycle(result)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Completed {i+1}/{n_cycles} cycles\")\n",
    "    \n",
    "    return monitor\n",
    "\n",
    "# Run test\n",
    "print(\"Running performance test...\")\n",
    "monitor = run_performance_test(n_cycles=50)\n",
    "\n",
    "# Compute and display statistics\n",
    "stats = monitor.compute_statistics()\n",
    "print(\"\\n=== PERFORMANCE STATISTICS ===\")\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Plot results\n",
    "monitor.plot_performance()\n",
    "\n",
    "# Generate report\n",
    "monitor.generate_report('test_results.json')\n",
    "print(\"\\nReport saved to test_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Repeatability Analysis (2 hours)\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Repeatability** measures how consistently a robot returns to the same position.\n",
    "\n",
    "**ISO 9283 Standard**: Repeatability specification\n",
    "- Execute same motion n times (typically n=30)\n",
    "- Measure position each time\n",
    "- Compute: $RP = \\bar{l} + 3S_l$\n",
    "  - $\\bar{l}$: Mean distance from centroid\n",
    "  - $S_l$: Standard deviation\n",
    "\n",
    "**Typical values**: \n",
    "- Industrial robots: ±0.05 mm\n",
    "- Collaborative robots: ±0.1 mm\n",
    "- Educational robots: ±0.5 mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_repeatability(robot, q_target, n_repeats=30):\n",
    "    \"\"\"\n",
    "    Measure positioning repeatability according to ISO 9283.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    robot : DHRobot\n",
    "    q_target : ndarray\n",
    "        Target joint configuration\n",
    "    n_repeats : int\n",
    "        Number of repetitions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    repeatability : float\n",
    "        Repeatability value (RP) in mm\n",
    "    positions : ndarray (n_repeats, 3)\n",
    "        Measured end-effector positions\n",
    "    \"\"\"\n",
    "    positions = []\n",
    "    \n",
    "    for i in range(n_repeats):\n",
    "        # Simulate positioning with noise\n",
    "        q_actual = q_target + np.random.normal(0, np.deg2rad(0.1), size=len(q_target))\n",
    "        \n",
    "        # Compute actual end-effector position\n",
    "        T = robot.fkine(q_actual)\n",
    "        pos = T.t\n",
    "        positions.append(pos)\n",
    "    \n",
    "    positions = np.array(positions)\n",
    "    \n",
    "    # Compute centroid\n",
    "    centroid = np.mean(positions, axis=0)\n",
    "    \n",
    "    # Distances from centroid\n",
    "    distances = np.linalg.norm(positions - centroid, axis=1)\n",
    "    \n",
    "    # ISO 9283 repeatability\n",
    "    l_bar = np.mean(distances)\n",
    "    S_l = np.std(distances)\n",
    "    RP = l_bar + 3 * S_l\n",
    "    \n",
    "    return RP * 1000, positions  # Convert to mm\n",
    "\n",
    "# TODO: Test repeatability at multiple poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.4: Multi-Point Repeatability Test\n",
    "\n",
    "Measure repeatability at various workspace locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workspace_repeatability_test(robot, test_points_configs):\n",
    "    \"\"\"\n",
    "    Test repeatability at multiple workspace locations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    robot : DHRobot\n",
    "    test_points_configs : list of ndarray\n",
    "        Joint configurations to test\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Repeatability data for each point\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'configurations': [],\n",
    "        'positions': [],\n",
    "        'repeatabilities': [],\n",
    "        'worst_case': None,\n",
    "        'best_case': None,\n",
    "        'mean_repeatability': None,\n",
    "    }\n",
    "    \n",
    "    print(\"Testing repeatability at multiple points...\")\n",
    "    \n",
    "    for i, q in enumerate(test_points_configs):\n",
    "        print(f\"Point {i+1}/{len(test_points_configs)}...\", end=' ')\n",
    "        \n",
    "        RP, positions = measure_repeatability(robot, q, n_repeats=30)\n",
    "        \n",
    "        # Get nominal position\n",
    "        T_nominal = robot.fkine(q)\n",
    "        pos_nominal = T_nominal.t\n",
    "        \n",
    "        results['configurations'].append(q)\n",
    "        results['positions'].append(pos_nominal)\n",
    "        results['repeatabilities'].append(RP)\n",
    "        \n",
    "        print(f\"RP = {RP:.3f} mm\")\n",
    "    \n",
    "    results['repeatabilities'] = np.array(results['repeatabilities'])\n",
    "    results['worst_case'] = np.max(results['repeatabilities'])\n",
    "    results['best_case'] = np.min(results['repeatabilities'])\n",
    "    results['mean_repeatability'] = np.mean(results['repeatabilities'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define test points for manufacturing cell\n",
    "test_configs = [\n",
    "    np.array([0, np.deg2rad(-45), np.deg2rad(45), 0]),  # Home\n",
    "    np.array([np.deg2rad(30), np.deg2rad(-20), np.deg2rad(30), 0]),  # Pick position\n",
    "    np.array([np.deg2rad(15), np.deg2rad(-30), np.deg2rad(40), 0]),  # Inspection\n",
    "    np.array([np.deg2rad(45), np.deg2rad(-25), np.deg2rad(35), 0]),  # Bin A\n",
    "    np.array([np.deg2rad(-45), np.deg2rad(-25), np.deg2rad(35), 0]), # Bin B\n",
    "]\n",
    "\n",
    "# TODO: Run repeatability test and analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: System Validation (2 hours)\n",
    "\n",
    "### Validation Against Specifications\n",
    "\n",
    "Compare actual performance vs requirements from SAE project specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System specifications from SAE project\n",
    "SPECIFICATIONS = {\n",
    "    'cycle_time_max': 10.0,  # seconds\n",
    "    'position_accuracy_max': 2.0,  # mm\n",
    "    'repeatability_max': 1.0,  # mm\n",
    "    'velocity_max': 200.0,  # mm/s\n",
    "    'jerk_max': 50.0,  # rad/s³\n",
    "    'success_rate_min': 0.98,  # 98%\n",
    "    'quality_accuracy_min': 0.95,  # 95%\n",
    "    'false_positive_max': 0.02,  # 2%\n",
    "    'false_negative_max': 0.01,  # 1%\n",
    "}\n",
    "\n",
    "def validate_system(performance_stats, repeatability_results, specifications=SPECIFICATIONS):\n",
    "    \"\"\"\n",
    "    Validate system performance against specifications.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    validation_report : dict\n",
    "        Pass/fail for each specification\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'tests': [],\n",
    "        'overall_pass': True,\n",
    "    }\n",
    "    \n",
    "    # Test 1: Cycle Time\n",
    "    test = {\n",
    "        'name': 'Cycle Time',\n",
    "        'spec': f\"<= {specifications['cycle_time_max']} s\",\n",
    "        'measured': performance_stats['cycle_time_mean'],\n",
    "        'pass': performance_stats['cycle_time_mean'] <= specifications['cycle_time_max'],\n",
    "    }\n",
    "    report['tests'].append(test)\n",
    "    if not test['pass']:\n",
    "        report['overall_pass'] = False\n",
    "    \n",
    "    # Test 2: Position Accuracy\n",
    "    test = {\n",
    "        'name': 'Position Accuracy',\n",
    "        'spec': f\"<= {specifications['position_accuracy_max']} mm\",\n",
    "        'measured': performance_stats['position_error_max'],\n",
    "        'pass': performance_stats['position_error_max'] <= specifications['position_accuracy_max'],\n",
    "    }\n",
    "    report['tests'].append(test)\n",
    "    if not test['pass']:\n",
    "        report['overall_pass'] = False\n",
    "    \n",
    "    # Test 3: Repeatability\n",
    "    test = {\n",
    "        'name': 'Repeatability',\n",
    "        'spec': f\"<= {specifications['repeatability_max']} mm\",\n",
    "        'measured': repeatability_results['worst_case'],\n",
    "        'pass': repeatability_results['worst_case'] <= specifications['repeatability_max'],\n",
    "    }\n",
    "    report['tests'].append(test)\n",
    "    if not test['pass']:\n",
    "        report['overall_pass'] = False\n",
    "    \n",
    "    # Test 4: Success Rate\n",
    "    test = {\n",
    "        'name': 'Success Rate',\n",
    "        'spec': f\">= {specifications['success_rate_min']*100}%\",\n",
    "        'measured': performance_stats['success_rate'],\n",
    "        'pass': performance_stats['success_rate'] >= specifications['success_rate_min'],\n",
    "    }\n",
    "    report['tests'].append(test)\n",
    "    if not test['pass']:\n",
    "        report['overall_pass'] = False\n",
    "    \n",
    "    # Test 5: Quality Classification Accuracy\n",
    "    test = {\n",
    "        'name': 'Quality Accuracy',\n",
    "        'spec': f\">= {specifications['quality_accuracy_min']*100}%\",\n",
    "        'measured': performance_stats['accuracy'],\n",
    "        'pass': performance_stats['accuracy'] >= specifications['quality_accuracy_min'],\n",
    "    }\n",
    "    report['tests'].append(test)\n",
    "    if not test['pass']:\n",
    "        report['overall_pass'] = False\n",
    "    \n",
    "    # Test 6: False Positive Rate\n",
    "    test = {\n",
    "        'name': 'False Positive Rate',\n",
    "        'spec': f\"<= {specifications['false_positive_max']*100}%\",\n",
    "        'measured': performance_stats['false_positive_rate'],\n",
    "        'pass': performance_stats['false_positive_rate'] <= specifications['false_positive_max'],\n",
    "    }\n",
    "    report['tests'].append(test)\n",
    "    if not test['pass']:\n",
    "        report['overall_pass'] = False\n",
    "    \n",
    "    # Test 7: False Negative Rate\n",
    "    test = {\n",
    "        'name': 'False Negative Rate',\n",
    "        'spec': f\"<= {specifications['false_negative_max']*100}%\",\n",
    "        'measured': performance_stats['false_negative_rate'],\n",
    "        'pass': performance_stats['false_negative_rate'] <= specifications['false_negative_max'],\n",
    "    }\n",
    "    report['tests'].append(test)\n",
    "    if not test['pass']:\n",
    "        report['overall_pass'] = False\n",
    "    \n",
    "    return report\n",
    "\n",
    "def print_validation_report(report):\n",
    "    \"\"\"\n",
    "    Print formatted validation report.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" \"*20 + \"SYSTEM VALIDATION REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Date: {report['timestamp']}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(f\"{'Test':<30} {'Specification':<20} {'Measured':<15} {'Result':<10}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for test in report['tests']:\n",
    "        result_str = \"✓ PASS\" if test['pass'] else \"✗ FAIL\"\n",
    "        measured_str = f\"{test['measured']:.4f}\"\n",
    "        print(f\"{test['name']:<30} {test['spec']:<20} {measured_str:<15} {result_str:<10}\")\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    overall_str = \"✓ SYSTEM PASSES ALL TESTS\" if report['overall_pass'] else \"✗ SYSTEM FAILED SOME TESTS\"\n",
    "    print(f\"\\nOVERALL: {overall_str}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# TODO: Run validation using collected performance data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.5: Complete Validation Test\n",
    "\n",
    "Run all tests and generate comprehensive validation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Run performance test\n",
    "print(\"Step 1: Running performance test...\")\n",
    "# TODO: Replace with actual manufacturing cell execution\n",
    "monitor = run_performance_test(n_cycles=50)\n",
    "perf_stats = monitor.compute_statistics()\n",
    "\n",
    "# Step 2: Run repeatability test\n",
    "print(\"\\nStep 2: Running repeatability test...\")\n",
    "# TODO: Create robot instance\n",
    "from roboticstoolbox import DHRobot, RevoluteDH\n",
    "robot = OpenManipulatorX()\n",
    "repeatability_results = workspace_repeatability_test(robot, test_configs)\n",
    "\n",
    "print(f\"\\nRepeatability Results:\")\n",
    "print(f\"  Best case: {repeatability_results['best_case']:.3f} mm\")\n",
    "print(f\"  Worst case: {repeatability_results['worst_case']:.3f} mm\")\n",
    "print(f\"  Mean: {repeatability_results['mean_repeatability']:.3f} mm\")\n",
    "\n",
    "# Step 3: Validate against specifications\n",
    "print(\"\\nStep 3: Validating system...\")\n",
    "validation_report = validate_system(perf_stats, repeatability_results)\n",
    "print_validation_report(validation_report)\n",
    "\n",
    "# Step 4: Save all results\n",
    "complete_report = {\n",
    "    'performance': perf_stats,\n",
    "    'repeatability': {\n",
    "        'best': float(repeatability_results['best_case']),\n",
    "        'worst': float(repeatability_results['worst_case']),\n",
    "        'mean': float(repeatability_results['mean_repeatability']),\n",
    "    },\n",
    "    'validation': validation_report,\n",
    "}\n",
    "\n",
    "with open('complete_validation_report.json', 'w') as f:\n",
    "    json.dump(complete_report, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\nComplete report saved to: complete_validation_report.json\")\n",
    "\n",
    "# Step 5: Generate visualizations\n",
    "monitor.plot_performance()\n",
    "\n",
    "print(\"\\nValidation test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Deliverables\n",
    "\n",
    "### What You Should Have Learned:\n",
    "\n",
    "1. **Unit Testing**: Creating comprehensive tests for kinematics and trajectories\n",
    "2. **Performance Metrics**: Collecting and analyzing KPIs\n",
    "3. **Repeatability**: ISO 9283 standard measurement\n",
    "4. **Validation**: Comparing performance vs specifications\n",
    "5. **Quality Reporting**: Professional documentation\n",
    "\n",
    "### Integration with SAE Project:\n",
    "\n",
    "For your final project:\n",
    "1. Implement complete test suite for all modules\n",
    "2. Run 50+ cycle performance test\n",
    "3. Measure repeatability at key workspace points\n",
    "4. Generate validation report\n",
    "5. Include test results in technical report\n",
    "\n",
    "### For the Technical Report:\n",
    "\n",
    "Add **Testing & Validation** section:\n",
    "- Test methodology\n",
    "- Performance metrics tables\n",
    "- Repeatability analysis graphs\n",
    "- Validation results vs specifications\n",
    "- Discussion of any failures and improvements\n",
    "\n",
    "---\n",
    "\n",
    "**Next Session**: Final integration and presentation preparation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
